{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT MINING DRUG REVIEWS DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Think About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can we create a way for people to find the best medication for their illness? \n",
    "In other words, can we use this dataset to implement a recommendation system?\n",
    "2. Is this problem better suited for classification or regression? \n",
    "In other words, what feature (or derived feature) should be used as the outcome variable?\n",
    "3. Can we determine what features or words are most important for predicting review rating or usefulness count?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORT SPARK AND CREATE A SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use findspak library to automatically locate spark installation for us to import it\n",
    "#install findspark\n",
    "#!pip install findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark and SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "spark = (SparkSession.builder\n",
    "                    .appName('medicineModel1')\n",
    "                    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read training data\n",
    "#header, delimiter are required options in this case\n",
    "train_df = spark.read.options(header=True, inferSchema=True, delimiter='\\t') \\\n",
    "  .csv('drugsComTrain_raw.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the first 10 lines of data\n",
    "train_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use head()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use take()\n",
    "train_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using limit(n).toPandas()\n",
    "train_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test data\n",
    "test_df = spark.read.options(header=True, inferSchema=True, delimiter='\\t') \\\n",
    "  .csv('drugsComTest_raw.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show some rows\n",
    "test_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print comlumn types\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#humber of rows in train_df\n",
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in test df\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of rows of combined data\n",
    "tot_num = train_df.count() + test_df.count()\n",
    "print(tot_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: PROCESS THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine train_df and test_df\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "#define function (takes a variable number of arguments)\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "#call the function\n",
    "combined_df = unionAll(train_df,test_df)\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of missing values in each column of the dataframe\n",
    "\n",
    "from pyspark.sql.functions import col,sum\n",
    "combined_df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined_df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with missing values based on the usefulCount column\n",
    "#use filter() method to return only rows for which usefulCount is not null\n",
    "combined_df1 = combined_df.filter(combined_df.usefulCount.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, count number of missing values in each column of the dataframe\n",
    "combined_df1.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined_df1.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with missing values in the condition column and check if there are still any missing values\n",
    "combined_df2 = combined_df1.filter(combined_df1.condition.isNotNull())\n",
    "combined_df2.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined_df2.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in cleaned df\n",
    "combined_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the first 5 rows of cleaned data\n",
    "combined_df2.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Date column data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse date into desired format\n",
    "def proc_date(date_str):\n",
    "    temp = date_str\n",
    "    temp1 = temp[temp.rfind(' ')+1:] #extract substring from index of \" \" plus 1 up to the end of the string\n",
    "    temp2 = temp.replace(\" \", \",\") #replace the space between month and day with a comma\n",
    "    temp3 = temp2[:-5] #extract month\n",
    "    temp4 = temp3+temp1 #concatenate the two substrings\n",
    "    return temp4\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the proc_date function\n",
    "my_date = 'November 3, 2015'\n",
    "my_date1 = proc_date(my_date)\n",
    "print(my_date1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the proc_date function to the data\n",
    "from pyspark.sql.types import IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "#lambda helper function\n",
    "prop_date_udf = udf(lambda date: proc_date(date), StringType())\n",
    "\n",
    "#transform the data using the above and save the result in new prop_date column as a string\n",
    "combined_df3 = combined_df2.withColumn(\"prop_date\", prop_date_udf(combined_df2.date))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define date formatter function\n",
    "from datetime import datetime\n",
    "def date_formatter(bad_date):\n",
    "    good_date = datetime.strptime(bad_date,'%B,%d,%Y').strftime('%Y-%m-%d')\n",
    "    return good_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the date_formatter function to the data\n",
    "from pyspark.sql.types import IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "#define lambda function\n",
    "sys_date_udf = udf(lambda p_date: date_formatter(p_date), StringType())\n",
    "\n",
    "combined_df4 = combined_df3.withColumn(\"sys_date\", sys_date_udf(combined_df3.prop_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute number of days\n",
    "#use pyspark's datediff library to compute number of days between two dates\n",
    "#problem: what will be the reference date?\n",
    "#Here using the bdate the dateset was donated to UCI MLL as reference date (i.e., October 4, 2018)\n",
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "combined_df5 = combined_df4.withColumn(\"num_days\", \n",
    "              datediff(to_date(lit(\"2018-10-04\")),\n",
    "                       to_date(\"sys_date\",\"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df5.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "drop_list = ['_c0','date', 'prop_date']\n",
    "\n",
    "#new df after dropping the columns\n",
    "combined_df6 = combined_df5.select([column for column in combined_df5.columns if column not in drop_list])\n",
    "combined_df6.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove quotation marks\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "combined_df6 = combined_df6.withColumn('review', regexp_replace('review', '\\\"', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new df after dropping the columns\n",
    "review1 = ['review']\n",
    "reviews = combined_df6.select([column for column in combined_df6.columns if column in review1])\n",
    "reviews.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_entities_udf = udf(lambda txt: BeautifulSoup(txt).text, StringType())\n",
    "combined_df6 = combined_df6.withColumn(\"review\", html_entities_udf(combined_df6.review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows containing \"</span>\"\n",
    "combined_df7 = combined_df6.filter(\"condition not like '%</span>%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df6.count(), combined_df7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df7.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with no usefulCount\n",
    "combined_df7a = combined_df7.filter(\"usefulCount > 0\")\n",
    "\n",
    "combined_df7.count(), combined_df7a.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a reverse map of indices and condition names\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "cond_name = combined_df7a.select('condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_name1 = [i.condition for i in combined_df7a.select('condition').distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cond_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond in cond_name1:\n",
    "    print(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_index = []\n",
    "j =0;\n",
    "for j in range(len(cond_name1)):\n",
    "    cond_index.append(j)\n",
    "    j +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cond_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in cond_index:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['condition', 'conditionID']\n",
    "\n",
    "condition_df = pd.DataFrame(zip(cond_name1,cond_index),columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_df.head(), condition_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_mapping = dict(condition_df[['condition', 'conditionID']].values)\n",
    "#print(cond_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_name = combined_df7a.select('drugName')\n",
    "drug_name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_name1 = [i.drugName for i in combined_df7a.select('drugName').distinct().collect()]\n",
    "len(drug_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_index = []\n",
    "k =0;\n",
    "for k in range(len(drug_name1)):\n",
    "    drug_index.append(k)\n",
    "    k +=1\n",
    "len(drug_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names1 = ['drugName', 'drugID']\n",
    "\n",
    "drug_df = pd.DataFrame(zip(drug_name1,drug_index),columns=col_names1)\n",
    "\n",
    "drug_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_mapping = dict(drug_df[['drugName', 'drugID']].values)\n",
    "#print(drug_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df8 = combined_df7a.toPandas()\n",
    "combined_df8['drugID'] = combined_df8.drugName.map(drug_mapping)\n",
    "combined_df8['conditionID'] = combined_df8.condition.map(cond_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "combined_df9 = sqlContext.createDataFrame(combined_df8)\n",
    "combined_df9.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_705 = combined_df9.filter(\"conditionID = 705\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = combined_df9.groupBy('conditionID').count().orderBy('count', asc=True)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df9.filter(\"conditionID = 705\").orderBy(\"rating\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df9_pd = combined_df9.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df9_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Helpfulness: Extremely helpful, Very helpful, Moderately helpful, Slightly helpful, Not at all helpful\n",
    "#Helpfulness_1: Extremely helpful, Very helpful, Helpful, Moderately helpful, Slightly helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels = [\"Slightly helpful\",\"Moderately helpful\",\"Helpful\",\"Very helpful\",\"Extremely helpful\"]\n",
    "combined_df9_pd['cat_usefulCount'] = pd.qcut(combined_df9_pd['usefulCount'], q=5, precision=0,\n",
    "                                             labels = cat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df9_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df9_pd['usefulCount_bins'] = pd.qcut(combined_df9_pd['usefulCount'], q=5, precision=0)\n",
    "\n",
    "combined_df9_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df9_pd.drop(['usefulCount_bins'], axis=1, inplace=True)\n",
    "combined_df10 = sqlContext.createDataFrame(combined_df9_pd)\n",
    "combined_df10.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"cat_usefulCount\", outputCol=\"usefulCountIndex\")\n",
    "indexed_df = indexer.fit(combined_df10).transform(combined_df10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = indexed.select('usefulCountIndex')\n",
    "y.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = y.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only numerical columns\n",
    "numeric_cols = ['rating','num_days', 'usefulCount']\n",
    "numeric_df = indexed_df.select([column for column in combined_df9.columns if column in numeric_cols])\n",
    "numeric_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "indexed_df = indexed_df.withColumn(\"rating\",col(\"rating\").cast(\"int\"))\n",
    "\n",
    "indexed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "columns_to_scale = [\"rating\", \"usefulCount\", \"num_days\"]\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
    "pipeline1 = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline1.fit(indexed_df)\n",
    "scaledData = scalerModel.transform(indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledData.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = scaledData.select('rating_scaled', 'num_days_scaled')\n",
    "\n",
    "numeric_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract text column\n",
    "#txt_col = ['review']\n",
    "#text_df = combined_df10.select([column for column in combined_df9.columns if column in txt_col])\n",
    "text_df = scaledData.select('review')\n",
    "text_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pineline for text processing\n",
    "from pyspark.ml.feature import CountVectorizer, StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "\n",
    "tokenized_df = tokenizer.transform(scaledData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"textFeatures\")\n",
    "\n",
    "transformed_df = remover.transform(tokenized_df)\n",
    "\n",
    "transformed_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"textFeatures\", outputCol=\"rawFeatures\", vocabSize=50000, minDF=2.0)\n",
    "\n",
    "cv_model = cv.fit(transformed_df)\n",
    "\n",
    "transformed_df2 = cv_model.transform(transformed_df)\n",
    "\n",
    "transformed_df2.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "idfModel = idf.fit(transformed_df2)\n",
    "transformed_df3 = idfModel.transform(transformed_df2)\n",
    "\n",
    "transformed_df3.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes\n",
    "nb_classes = transformed_df3.select('usefulCountIndex').distinct().count()\n",
    "\n",
    "#number of inputs or input dimensions\n",
    "input_dim = len(transformed_df3.select('features').first()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb_classes, ',', input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "#shuffle the data\n",
    "final_df = transformed_df3.orderBy(rand())\n",
    "\n",
    "#split the data\n",
    "train_data, test_data = final_df.randomSplit((0.75, 0.25), seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rows to use\n",
    "train_df = train_data.select('rating_scaled','num_days_scaled', 'features', 'usefulCountIndex')\n",
    "test_df = test_data.select('rating_scaled','num_days_scaled', 'features', 'usefulCountIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Dataset Count: \" + str(train_df.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras deep learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df9['use_count_bucks'] = pd.qcut(combined_df9['usefulCount'], q=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data samples\n",
    "X_train = train_df.select(\"rating_scaled\", \"num_days_scaled\", \"features\")\n",
    "y_train = train_df.select(\"usefulCountIndex\")\n",
    "\n",
    "X_test = test_df.select(\"rating_scaled\", \"num_days_scaled\", \"features\")\n",
    "y_test = test_df.select(\"usefulCountIndex\")\n",
    "\n",
    "nlp_input = final_df.select(\"features\")\n",
    "numeric_input = final_df.select(\"rating_scaled\", \"num_days_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "#which allows you to have multiple inputs and indirect connections.\n",
    "embedding_size =64\n",
    "seq_length = 100\n",
    "\n",
    "nlp_input = Input(shape=(seq_length,), name='nlp_input')\n",
    "numeric_input = Input(shape=(2,), name='numeric_input')\n",
    "emb = Embedding(output_dim=embedding_size, input_dim=50000, input_length=seq_length)(nlp_input)\n",
    "nlp_out = Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01)))(emb)\n",
    "x = concatenate([nlp_out, numeric_input])\n",
    "x = Dense(classifier_neurons, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[nlp_input , numeric_input], outputs=[x])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(4, 8))\n",
    ")\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
